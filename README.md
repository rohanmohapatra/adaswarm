# AdaSwarm

A novel gradient-free optimizer which has similar or even better performance thanthe Adam optimizer adopted in neural networks.

## EMPSO

EMPSO simulates gradient descent with momentum and is inspired from the back-propagation algorithm with momentum in neural networks. The proposed optimizing algorithm constantly gives faster convergence time in all the available test optimization functions (constrained or unconstrained).
The published paper can be found [here](https://www.springerprofessional.de/en/a-new-approach-for-momentum-particle-swarm-optimization/18215730).

It is developed using PyTorch and the implementation is available on [GitHub](https://github.com/rohanmohapatra/torchswarm) and a published [pypi package](https://pypi.org/project/torchswarm/).

## Publication

This paper is published at [IEEE Transactions on Emerging Topics in Computational Intelligence](https://ieeexplore.ieee.org/document/9472873).

### Authors

| Author                  |              Email               |
| ----------------------- | :------------------------------: |
| Rohan Mohapatra         |    rohannmohapatra@gmail.com     |
| Snehanshu Saha          |   scibase.snehanshu@gmail.com    |
| Carlos A. Coello Coello |     ccoello@cs.cinvestav.mx      |
| Anwesh Bhattacharya     | 2016590@pilani.bits-pilani.ac.in |
| Soma S. Dhavala         |      soma.dhavala@gmail.com      |
| Sriparna Saha           |     sriparna.saha@gmail.com      |
